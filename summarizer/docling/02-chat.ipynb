{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603889ff-e355-4c9e-929a-45e8150aa0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61faf199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b694cfeb-1653-4555-84ce-875b3885150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(\n",
    "            \">>>{}: The output is: {}. \\n\\nThe metadata is {} \\n\\n\".format(\n",
    "                i + 1,\n",
    "                (\n",
    "                    doc.page_content[:100] + \"...\"\n",
    "                    if len(doc.page_content) > 100\n",
    "                    else doc.page_content\n",
    "                ),\n",
    "                doc.metadata,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0788aef1-0fe8-4205-ba3d-741bb5cf392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "db = FAISS.load_local(\n",
    "    \"summarizer_index\", embeddings_model, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94184d22-46ad-45e7-8a00-38c298b7e607",
   "metadata": {},
   "source": [
    "## Perform semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82dd70e3-e3ca-4b2d-a238-3ed466e41618",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "worked with partners\n",
    "\"\"\"\n",
    "docs = db.similarity_search(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff62b2cc-cebc-43ab-a2ff-d785f40ca73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>1: The output is: |    |                                                                                              .... \n",
      "\n",
      "The metadata is {} \n",
      "\n",
      "\n",
      ">>>2: The output is: Forever grateful to the big Nokia family for this opportunity, it was worth meeting you and I hope y.... \n",
      "\n",
      "The metadata is {} \n",
      "\n",
      "\n",
      ">>>3: The output is: |    | relevant articles that match the partner's                                                   .... \n",
      "\n",
      "The metadata is {} \n",
      "\n",
      "\n",
      ">>>4: The output is: | 2                                                    | Contribute to the release of the first team.... \n",
      "\n",
      "The metadata is {} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_output(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b513bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e071684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ibm-granite/granite-3.0-2b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66072554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU device with no GPU\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# device_id = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29a8f29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598a973baa8548619ac75bb7ab87ea2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metal Performance Shaders (MPS) for Apple GPUs\n",
    "device_id = \"mps\"\n",
    "device = torch.device(device_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edc12b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device_id,\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,  # important to avoid generation errors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f69b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmartino/projects/AI/learning/chat-with-your-data-using-chatgpt-3806110/venv/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"human\",\n",
    "#             \"\"\"You are an assistant for question-answering tasks. \n",
    "# Use the following pieces of retrieved context to answer the question. \n",
    "# If you don't know the answer, just say that you don't know. \n",
    "# Use four sentences maximum and keep the answer concise.\n",
    "# Question: {question} \n",
    "# Context: {context} \n",
    "# Answer: <your answer here>\n",
    "# Word count: <total word count>\n",
    "# \"\"\",\n",
    "#         )\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb9901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8fbec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# combine multiple steps in a single chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()  # convert the chat message to a string\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57b73dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "You are the person for which we provide the development and individual item reports\n",
    "and the LinkedIn profile description.\n",
    "Generate a profile description for yourself.\n",
    "The profile must include:\n",
    "- one paragraph with the description of professional role, highlighting the competences\n",
    "- one paragraph with a comma separated list of recent achievements\n",
    "- one paragraph with a comma separated list of skills\n",
    "\n",
    "Use first person as \"I am\" instead of \"He is\".\n",
    "Use generic terms instead of specific examples: \"DevOps\" instead of \"Jenkins\".\n",
    "Avoid to mention customer names like Temenos.\n",
    "Use skills categorizies when possible.\n",
    "At the end of each sentence add a list of the reference source files.\n",
    "At the end of the response, include a count of words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ab3171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Professional Role:\n",
      "\n",
      "I am a Principal Software Engineer at Red Hat, with a strong focus on software architecture, design, and development. I have expertise in various technologies such as Java, Spring, Hibernate, REST, and Agile (Scrum). I am also proficient in TDD and MDD. My role involves leading a development team of up to 200+ people worldwide and continuously earning their trust through my technical excellence and proven track record of top-ranking performance evaluations.\n",
      "\n",
      "Recent Achievements:\n",
      "\n",
      "1. Learned about the Red Hat product portfolio, gaining a good understanding of its features and capabilities, particularly OpenShift Container Platform.\n",
      "2. Understood Red Hat divisions, relationships, and organizations, as well as the Open Source development process and productization concerns.\n",
      "3. Completed C. Wright's learning courses on AI and started Feast-related trainings.\n",
      "4. Started using the Red Hat learning day to collaborate on selected projects.\n",
      "5. Created comprehensible and appealing presentations, understanding the real needs of partners, and translating them into recommended Red Hat architectures and solutions.\n",
      "\n",
      "Skills:\n",
      "\n",
      "1. Software Architecture, Design, and Development\n",
      "2. TDD\n",
      "3. MDD\n",
      "4. Java\n",
      "5. Spring\n",
      "6. Hibernate\n",
      "7. REST\n",
      "8. Agile (Scrum)\n",
      "9. Continuous Learning\n",
      "10. Execution\n",
      "11. Problem Solving\n",
      "12. Influence\n",
      "13. Portfolio Excellence\n",
      "14. Red Hat Organization and Dynamics\n",
      "15. Open Source Development Process and Productization Concerns\n",
      "16. Effective Communication\n",
      "17. Cloud Architecture Knowledge (Azure Cloud Services)\n",
      "\n",
      "Word Count: 150"
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\n",
    "    question\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac5cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf18f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
